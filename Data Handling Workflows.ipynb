{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Handling Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Note that Parsl is not effective if multiple CPU cores aren't available because Parsl's ability to execute tasks in parallel is depenedent on the availability multiple cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores available: 4\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "print('Cores available: {}'.format(multiprocessing.cpu_count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<parsl.dataflow.dflow.DataFlowKernel at 0x118da8d90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import parsl\n",
    "import os\n",
    "import montage_wrapper as montage\n",
    "from parsl.data_provider.files import File\n",
    "cwd = os.getcwd()\n",
    "\n",
    "from parsl.app.app import python_app, bash_app\n",
    "from parsl.providers import LocalProvider\n",
    "from parsl.channels import LocalChannel\n",
    "\n",
    "from parsl.config import Config\n",
    "from parsl.executors import HighThroughputExecutor\n",
    "\n",
    "config = Config(\n",
    "    executors=[\n",
    "        HighThroughputExecutor(\n",
    "            label=\"htex_local\",\n",
    "            cores_per_worker=1,\n",
    "            provider=LocalProvider(\n",
    "                channel=LocalChannel(),\n",
    "                init_blocks=1,\n",
    "                max_blocks=1,\n",
    "            ),\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "parsl.load(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A map reduce is a technique to execute multiple parallel jobs on a dataset to reduce the size of the dataset before executing a final function to get the result. A Map reduce is a more complicated version of synchronisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a simple example where we are given multiple lists and we want to select the lists with the highest standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/map_reduce.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@python_app\n",
    "def standard_deviation(inputs=[]):\n",
    "    '''\n",
    "    A python app to compute the standard deviation of the inputs\n",
    "    '''\n",
    "    import numpy as np\n",
    "    return np.std(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data():\n",
    "    '''\n",
    "    A function to construct data that is a list of lists, each each list having 100 random numbers.\n",
    "    '''\n",
    "    lists = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "        new_list = []\n",
    "        \n",
    "        for __ in range(100):\n",
    "            new_list.append(random.random()*100)\n",
    "            \n",
    "        lists.append(new_list)\n",
    "    return lists\n",
    "\n",
    "our_data = make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Standard Deviation:  31.65997545363917\n",
      "Target List Number: 44\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Computing the standard deviations for each list\n",
    "'''\n",
    "\n",
    "start1 = time.time()\n",
    "\n",
    "standard_deviations = []\n",
    "\n",
    "for i in our_data:\n",
    "    standard_deviations.append(standard_deviation(inputs=i))\n",
    "\n",
    "'''\n",
    "Finding the maximum standard deviation\n",
    "'''\n",
    "\n",
    "standard_deviations = [i.result() for i in standard_deviations]\n",
    "print('Maximum Standard Deviation: ', max(standard_deviations))\n",
    "\n",
    "'''\n",
    "Finding the list with the maximum standard deviation\n",
    "'''\n",
    "\n",
    "maximum = max(standard_deviations)\n",
    "print('Target List Number:', standard_deviations.index(maximum))\n",
    "\n",
    "end1 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using a simple hash function to store elements in our database. We'll evaluate the hash values in parallel and then store the items in those locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "An empty database\n",
    "'''\n",
    "database = [0 for i in range(1000)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@python_app \n",
    "def hash_function(element):\n",
    "    '''\n",
    "    We import the haslib library and then create the hash index.\n",
    "    '''\n",
    "    \n",
    "    import hashlib\n",
    "    number = int(hashlib.md5(element).hexdigest()[:8], 16)%1000\n",
    "    return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "elements = []\n",
    "\n",
    "for i in range(100):\n",
    "    '''\n",
    "    Making a 5 letter element and collecting 100 such elements.\n",
    "    '''\n",
    "    element = '' \n",
    "    for _ in range(5):\n",
    "        element += random.choice('abcdefghijklmopqrstuvwxyz')\n",
    "    element = element.encode()\n",
    "    elements.append(element)\n",
    "\n",
    "start1 = time.time()   \n",
    "hashes = []\n",
    "for i in elements:\n",
    "    '''\n",
    "    Updating the database for all the elements.\n",
    "    '''\n",
    "    hashes.append(hash_function(i))\n",
    "\n",
    "hashes = [i.result() for i in hashes]\n",
    "\n",
    "for i in range(len(elements)):\n",
    "    database[hashes[i]] = elements[i]\n",
    "\n",
    "end1 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this still doesn't solve the problem of overlap of elements. Chaining is the alternative here but in order to implement chaining, we have to evaluate the results which breaks the parallel thread."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
